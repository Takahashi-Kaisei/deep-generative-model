{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# フローベースモデル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "潜在変数から観測変数は，決定論的な写像 & 可逆であるとする．\n",
    "\n",
    "潜在変数と観測変数は，全単射の関係である．てことは潜在変数の次元と観測変数の次元は等しい．\n",
    "\n",
    "わかっているのは，潜在変数の確率分布と，変換器$f$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "確率密度関数の変数変換の公式によって，潜在変数の確率密度関数から，観測変数の確率密度関数を求めることができる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p(x)dx = p(y)dy\n",
    "$$\n",
    "確率密度関数の積分は，変数変換前後で等しい．\n",
    "\n",
    "確率密度関数なので，変数を変換した後でも，総面積は変わらない．それが微少量に着目していたとしてもね．\n",
    "\n",
    "んでこの式を変形すると，$$p(y) = p(x) \\left| \\frac{dx}{dy} \\right|$$となる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "では，これの対数尤度を考える．\n",
    "\n",
    "$$\n",
    "\\log p(y) = \\log p(x) + \\log \\left| \\frac{dx}{dy} \\right|\n",
    "$$\n",
    "\n",
    "これだけ，多変量だとして，ヤコビアンを導入すると，\n",
    "\n",
    "$$\n",
    "\\log p(\\boldsymbol{x}) = \\log p(f^{-1}(\\boldsymbol{x})) + \\log \\left| \\det \\frac{df^{-1}(\\boldsymbol{x})}{d\\boldsymbol{x}} \\right|\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "フローベースモデルのジレンマ\n",
    "\n",
    "- $f$をリッチにしたい．\n",
    "    - しかし，fは可逆である必要がある．\n",
    "    - ヤコビ行列式が計算できなければならない．\n",
    "\n",
    "これをどう解決するかがフローベースモデルのテーマとなる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 正規化フロー\n",
    "f自体は，単純な変換しかできないので，単純な変換を繰り返し適用(関数の合成)をすることで複雑な分布を表現する．\n",
    "$$\n",
    "\\boldsymbol{x} = \\boldsymbol{z}_K = f_K \\circ f_{K-1} \\circ \\cdots \\circ f_1(\\boldsymbol{z}_0)\n",
    "$$\n",
    "これを，パスと呼ぶ．\n",
    "\n",
    "可逆な$f$による$z_0$から合成変換のパスのことをフローと言い，フローによって有効な分布が構成される場合は正規化フローと言う．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\boldsymbol{x}$をサンプリングするためには，$\\boldsymbol{z}_0$をサンプリングする．\n",
    "\n",
    "で，最初0の分布が簡単なので，サンプリングは容易．フローに通しちゃえばいいだけだから．\n",
    "\n",
    "それは以下．\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\log p(\\boldsymbol{x}) = \\log p(\\boldsymbol{z}_0) &= \\log p_{K-1}(\\boldsymbol{z}_{K-1}) - \\log \\left| \\det \\frac{df_{K}}{d \\boldsymbol{z}_{K-1}} \\right| \\\\\n",
    "&= \\log p_{K-2}(\\boldsymbol{z}_{K-2}) - \\log \\left| \\det \\frac{df_{K-1}}{d \\boldsymbol{z}_{K-2}} \\right| - \\log \\left| \\det \\frac{df_{K}}{d \\boldsymbol{z}_{K-1}} \\right| \\\\\n",
    "&= \\cdots \\\\\n",
    "&= \\log p_0(\\boldsymbol{z}_0) - \\sum_{i=1}^{K} \\log \\left| \\det \\frac{df_i}{d \\boldsymbol{z}_{i-1}} \\right|\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "各層で$\\log$ヤコビアンを計算する．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "フローを用いることのメリットは，\n",
    "\n",
    "推論が，逆変換を用いてできること．変換を遡れる．\n",
    "\n",
    "$x$が与えられたときに，$z$を求めることができる．\n",
    "\n",
    "今まではVAEだったらエンコーダを作ったりとかで工夫した．GANだったらdiscriminatorを作ったりとかで工夫した．\n",
    "\n",
    "要は二つモデルを作らなければいけなかったんよね．\n",
    "\n",
    "だけどフローベースであれば，可逆なのでモデルは一個で良い．\n",
    "\n",
    "推論を$f$，瀬性を$f^{-1}$で表現できる．\n",
    "\n",
    "そのようなモデル$f$を設計するためには，\n",
    "- 可逆であること．(全単射)\n",
    "- ヤコビアンが簡単に計算できること．\n",
    "\n",
    "このような$f$をニューラルネットワークでどう表現するかが課題である．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f$はどうモデリングすれば良いのだろうか．\n",
    "\n",
    "まず，$f$は単純で良いのだが，$x$の次元×$x$の次元のヤコビアンを計算することは変わらない．\n",
    "\n",
    "これをどう楽していこうかね．\n",
    "\n",
    "- 三角行列\n",
    "\n",
    "ヤコビ行列が三角行列であれば，行列式は対角成分の積で求められる．\n",
    "\n",
    "よって，ヤコビ行列が三角行列になるような可逆変換を考える．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- カップリング層\n",
    "\n",
    "入力$\\boldsymbol{x}\\in \\mathbb{R}^D$を，二つのブロックに分ける．\n",
    "- $\\boldsymbol{x} = [\\boldsymbol{x}_{1:d}:\\boldsymbol{x}_{d+1:D}] = [\\boldsymbol{x}_{I_1}:\\boldsymbol{x}_{I_2}]$\n",
    "\n",
    "それぞれのブロックに対して，次のような変換を考える．(カップリング層)\n",
    "\n",
    "$$\n",
    "\n",
    "\\begin{align*}\n",
    "\\boldsymbol{y}_{I_1} &= \\boldsymbol{x}_{I_1} \\\\\n",
    "\\boldsymbol{y}_{I_2} &= g(\\boldsymbol{x}_{I_2} ; m(\\boldsymbol{x}_{I_1}))\n",
    "\\end{align*}\n",
    "\n",
    "$$\n",
    "\n",
    "- $g:\\mathbb{R}^{D-d} \\times \\mathbb{R}^d \\to \\mathbb{R}^{D-d}$は可逆関数\n",
    "- $m$は任意の関数(カップリング関数と言う)\n",
    "\n",
    "この変換をすると，三角行列になる，ヤコビ行列式が計算しやすくなる．\n",
    "\n",
    "ポイント：逆変換において，$g$が可逆である必要はあるが，$m$は可逆である必要はない，だからDNNとかを使えちゃう．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NICE(Non-linear Independent Component Estimation) 2014年あたり．\n",
    "\n",
    "加法カップリングを利用したモデル．($g(\\boldsymbol{a};\\boldsymbol{b})=\\boldsymbol{a}+\\boldsymbol{b}$)\n",
    "\n",
    "-この中は省略-\n",
    "\n",
    "結果的にヤコビ行列式は1になる．\n",
    "\n",
    "変換前後で体積が不変であるということ．\n",
    "\n",
    "学習がめっちゃシンプルになるのは良いが，フローの過程で分布が変化していないのと同義になる．\n",
    "\n",
    "- NICEの工夫\n",
    "    - スケーリング行列を導入(対角行列$S$を導入，行列式が対角成分の積になる．)\n",
    "\n",
    "$$\n",
    "\\log p(\\boldsymbol{x}) = \\sum_{i=1}^{D}[\\log p_i(f_i(\\boldsymbol{x}) + \\log \\left| S_{ii} \\right|]\n",
    "$$\n",
    "\n",
    "- 潜在変数の事前分布\n",
    "    - VAEなどと同様，ガウス分布や，ロジスティック分布などを使用．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RealNVP(Real-valued Non-Volume Preserving) 2017年あたり．\n",
    "\n",
    "アフィンカップリングを利用\n",
    "- $g(\\boldsymbol{a};\\boldsymbol{b})=\\boldsymbol{a} \\odot \\boldsymbol{b}_s + \\boldsymbol{b}_t, \\boldsymbol{b}_s = \\exp(s(\\boldsymbol{x}_{I1})), \\boldsymbol{b}_t = t(\\boldsymbol{x}_{I1})$\n",
    "- ただし，$s\\mathbb{R}^{d} \\to \\mathbb{R}^{D-d}, t:\\mathbb{R}^{d} \\to \\mathbb{R}^{D-d}$\n",
    "    - $s$や$t$にはDNNなどを用いる．\n",
    "\n",
    "アフィンカップリングによるカップリング層は，...略\n",
    "\n",
    "NICEと異なり，行列式は1ではない．変換前後で体積が変わっている(分布がちゃんと変化している)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基本的な考え方．\n",
    "\n",
    "- ヤコビ行列を対角行列にしたい．\n",
    "    - そのためにカップリング層を導入する．\n",
    "       - その中で$g$や$m$を考える．\n",
    "            - $g$は可逆にしたい(シンプルにしたい．)\n",
    "                - シンプルしすぎると分布が変わらないので，いい感じに工夫する．(具体的には，アフィン変換とかね．)\n",
    "            - $m$はなんでもいい．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "問題があって$x_1, x_2$に2ブロックに分割したとき，片方は変換がかかって，もう片方はかからないよね．\n",
    "\n",
    "これは，半分が変換されて，半分が変換されないって感じでよくない．\n",
    "\n",
    "これを解決するために，入れ替える．\n",
    "\n",
    "交互に変換するって感じ．\n",
    "\n",
    "片方を固定して，片方を変換．それを繰り返す．\n",
    "\n",
    "これはマスキングとも捉えられる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Multi-Scale Architecture\n",
    "\n",
    "交互のカップリング$K$回を行う．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Glow 2018年あたり．\n",
    "\n",
    "Multi-Scale Architectureと同じなんだけど，複雑に交互にマスキングとかはしない，シンプルにした．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- フローベースモデルのまとめ\n",
    "- 利点\n",
    "    - 厳密な推論，対数尤度の評価ができる．\n",
    "        - 既存の深層生成モデルでは困難\n",
    "        - 画像生成以外いにも，異常検知，幅広い観点での利用が期待．\n",
    "    - 推論，生成が効率的，(可逆)\n",
    "    - 潜在変数内での操作ができる．\n",
    "- 欠点\n",
    "    - アーキテクチャに制限がある．\n",
    "        - 逆変換やヤコビ行列の計算が容易である層を利用する必要がある．\n",
    "        - 層を増やしても次元は変わらない．\n",
    "    - 各フローでヤコビ行列などをとっておく必要があるので，全体としてメモリを食う．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自己回帰モデル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\boldsymbol{x}=[x_1, \\ldots, x_D]$について，尤度を条件付き分布の積で構成．\n",
    "\n",
    "$$\n",
    "p(\\boldsymbol{x}) = \\prod_{d=1}^{D} p(x_d | x_{<d})\n",
    "$$\n",
    "\n",
    "フローベース同様，厳密に尤度を計算できる．\n",
    "\n",
    "これをニューラルネットで構築したいわね．\n",
    "\n",
    "でも，サンプリングが超大変．$x_1$から順番にサンプリングしないといけない．次元が大きくなると，画像の左上からサンプリングするみたいになって大変だね．\n",
    "\n",
    "自己回帰モデルでは，どのようにっ条件付き分布を効率的に計算できるかがポイント．\n",
    "\n",
    "1ピクセルごとに計算するのはきつい．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MADE(Masked Autoencoder for Distribution Estimation)\n",
    "\n",
    "オートエンコーダの各層にマスクをかけて，先の要素以外から条件づけられるように構成する．\n",
    "\n",
    "- 隠れ層に適当な番号をつける．\n",
    "- 自分自身の番号より大きいユニットからの経路を遮断するようにマスクを1にする．\n",
    "\n",
    "→一回の順伝播で，自己回帰の全条件付き分布の出呂0区を得ることができる．(めっちゃすごい．これまではこれがきつかった)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PixelCNN/PixelRNN\n",
    "\n",
    "画像を上から下，左から右に生成する．\n",
    "- 自分より前のピクセルが文脈，条件となる．\n",
    "- 文脈だけを見る方法は3種類提案．\n",
    "    - PixelCNN\n",
    "        - マスク付きの畳み込み．\n",
    "            - 高速に計算できるものの，文脈を全部見ることができないため，尤度の推定精度が低い．\n",
    "    - PixelRNN\n",
    "        - Row LSTM\n",
    "            - 上から下に1D Conv+LSTMを使う．\n",
    "            - 上方向の文脈は全て見ることができるが，速度が遅い．\n",
    "        - Diagonal BiLSTM\n",
    "            - 正方形に文脈を見るために，各行を1ピクセルずつずらした後に1D Conv+LSTM(Sekwing)\n",
    "            - 文脈を全てみれるが，Row LSTMよりも速度が遅い．\n",
    "\n",
    "同じ文脈を保って多層構造にする．\n",
    "\n",
    "これらは，高次元画像を生成できるが，一貫性を捉えることができていない．\n",
    "\n",
    "- WaveNet\n",
    "    - 1-Dの音声信号に対する，pixelCNNに基づく自己回帰モデル\n",
    "        - 綺麗な音声合成ができるということで話題．\n",
    "    - Casual Convolution\n",
    "        - pixelCNNと同様に，未来の情報を使わないようにする．\n",
    "        - ただし，マスクではなく，単純にタイムステップの分をずらす．\n",
    "    - Dilated Convolution\n",
    "        - convだと限られた受容野からしか情報を受け取れない．\n",
    "        - そこで，広い範囲から飛び飛びに入力をとって畳みこむ．\n",
    "            - 層が大きいほど飛ぶ大きさを大きくする．\n",
    "\n",
    "自己回帰モデルでは，いかに遠いところからの情報を持ってくるかで工夫をしていた．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自己回帰フロー"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

# 生成モデルのおさらい
## 生成モデルができること．
### 生成
### 密度推定(外れ値検知，異常検知)
### 欠損値補完．ノイズ除去
## 推論
### 生成，推論
潜在変数モデルから，観測変数を推定する．
右向きの矢印．
### 推論
観測変数から，潜在変数を推し量る．
左向きの矢印．
一般的に計算困難．
## 識別モデルとの違い．
識別モデルは条件付き確率のみ．しかし，生成モデルは同時分布がわかるので．条件付き確率も算出できる．

# VAE(変分自己符号化器)
## 特徴．
### 尤度計算としては，対数尤度の下界が算出できる．

計算困難の理由．
logの中に積分記号が入ることがだめ．
p_θのモデルは今回ニューラルネットワークなので，厳密な積分が計算困難．

具体的には，
ELBO(エビデンス下界)は，$\text{log}p_{\theta}(x)$と KLダイバージェンスに分けた時に，KLダイバージェンスに事後分布の計算式が出てくる．
それも，推論についての...
事後分布を計算するために，ベイズの定理を用いて周辺化するのだが，その時に．
$p_{\theta}(z|x)=\frac{p_{\theta}(x,z)}{\int p_{\theta}(x,z)dz}$
という式が出てくる．この分母は解析的に解けないので，事後分布は計算困難となる．
生成モデルが複雑だと分母の周辺化が解析的に解けない．


そこで，適当な分布$q(z)$を，$ p(z)$ を $p_{\phi}(z|x)$という推論モデルに限定する．学習可能にするってこと．

VAEの学習
φやθはそのままDNNの重みを使用すればOK．
よってELBOが目的関数になって，ニューラルネットワークを学習すれば良いということになる．
KLダイバージェンスは解析的に解けるのでOK．しかしあまりもののELBOの第一項は，サンプリングが必要になる．
この時，潜在変数はガウス分布に従うとして，サンプリングする．(潜在変数を，パラメータφに従う，潜在変数の条件付き分布から．)
ここで問題なのが，学習ができなくなること．
具体的には，ニューラルネットワークを使っているので，誤差逆伝播法でパラメータを更新したいのだけれど，サンプリングする過程が存在するので，誤差逆伝播法ができない．途切れてしまう．
よって以下を行う．

再パラメータ化トリック
普通の誤差逆伝播は，勾配の情報を保持しておいて，それを逆向きに流すことで更新する．
しかし，サンプリングがあると，途中で演算がない(計算ではなくサンプリングする)ので，勾配の情報が途切れてしまう．
そこで，演算によってサンプリングする．
ガウス分布を言い換えると．基本的には平均に従うんだけど，たまにズレる．ような分布と言い換えられる．
これを演算に換える．
$z=\mu+\sigma \odot \epsilon$
$\epsilon \sim N(0,1)$
これで擬似的なサンプリングができる．

VAEのELBO
ELBO=負の再構成誤差 + KLダイバージェンス
負の再構成誤差は，入力された画像を潜在変数に圧縮．(ここは推論モデル)潜在変数を観測変数に再構成(生成モデル)
KLダイバージェンスは，正則化項として働き，推論モデルと単純なガウス分布を近づける．ということをしている．
推論モデルがエンコーダ．生成モデルがデコーダの役割．

VAEの弱点
輪郭がぼやけてしまう．




VAEと表現学習
VAEは，再構成(デコード)だけでなく，表現$z \sim q_{\phi}(z|x)$も学習しているとみなせる．
表現学習とは，高次元を，低次元にコンパクトにすること．
人間は，猫を見た時，高次元の猫の画像から，"ねこ"というコンパクトな情報に変換している．

良い表現とは？
他のタスクにも使いまわせるような表現．
いきなりだがMeta-Priorという話が出てくる．
同時に多くのタスクに使える表現の性質に関する仮定．
 多様体など...
要は良い表現はどのような性質を持つか．という仮定かな．

多様体
多様体学習．
潜在空間の可視化

Disentangled Representation(もつれを解く表現)
データは，独立にエンカする変動の要因から生成される．
例：猫の画像があった時,,,猫の種類，猫の態勢，光源の角度とか
それぞれのこの要素を独立とし，切り分けて推論してあげることで，猫の種類という表現．猫の態勢という表現，光源の角度という表現を独立に獲得できる．
VAEはこれが得意．

VAEとDisentangled Representation
ELBOの第二項であるKLダイバージェンスがpとqを近づけるんだけど．その行為が独立させる．という意味になっている．
要は正則化の役割になってるんだね．
てことは，正則化の強度が変えられれば，違った出力が出せるようになるね．

正則化項を強くすると→より独立した表現が得られる．
正則化項を弱くすると→より多様な表現が得られる．(独立してないってこと．)

正則化項の分解．
正則化項で一体何が起こっているのだろうか？

posterior collapse
再構成誤差項と，xとzの相互情報量の項は競合していて，トレードオフになっている．
相互情報量0は，zがxに対して情報を持っていないということ．
これを防ぐために，正則化項を強くする．






自己教師あり学習
自己教師あり学習は，自分で画像にマスキングしたりなどして，自分で教師を作る．
VAEも，いい表現を，教師なしで学習している．
生成モデルは自己教師あり学習とみなせる．







CVAE(Conditional VAE)
条件付きVAE
推論モデルと生成モデル両方に，yという条件を加えてあげる．
今までの条件付き分布の中に新しくyという条件も入れるということ．
今まで，観測変数から，潜在変数に押し込む．というのをやってたんだけど，その場合，観測変数の情報「すべて」を潜在変数にエンコードできないといけない．そこで，yをあげることで，潜在変数が，y以外の別の表現を獲得することができる．
MNISTなら，yに数字ラベルを与えることで，潜在変数は筆跡，文字の太さなどの情報にフォーカスすることができる．
さらには，yに自然言語を入力して画像を生成することもできる．

ラベルが手に入らない時．
VAEによる半教師あり学習
略


潜在変数の階層化
潜在変数の潜在変数を仮定する．
Ladder VAE
潜在変数を階層化することで，より複雑な表現を獲得できる．
条件付き分布を積でくくる．
Nouveau VAE
生成モデルと，推論モデルでresnetを使う．



潜在変数の離散化
Vector Quantized VAE
通常のVAEでは潜在変数は連続，それを離散にする．
積分だったところが和になる．
今までと同じ流れで再パラメータ化トリックを使いたいんだけど，離散だと使えない．
→そこで推論モデルを工夫する．

決定論的な推論モデル

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分配関数の計算を避けたい．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ネガティブサンプルに対する勾配の計算は不要．\n",
    "\n",
    "スコア関数自体が対数尤度の勾配の形なので，ランジュバンダイナミクスによる勾配の計算は不要．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EBMでは，分配関数の計算のために，negative samplingをしていた．\n",
    "\n",
    "結局はそこがボトルネックとなっており，ランジュバンダイナミクスなどの手法で，サンプリングしていた．\n",
    "\n",
    "そこを改善したいのがSBM．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず，EBMの尤度(厳密には負の対数尤度)の勾配を確認する．\n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta}\\mathbb{E}_{\\boldsymbol{x} \\sim p_{data}}[-\\log p_{\\theta}(\\boldsymbol{x})] = \\mathbb{E}_{\\boldsymbol{x} \\sim p_{data}}[\\nabla_{\\theta} E_{\\theta}(\\boldsymbol{x})]-\\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\theta}}[\\nabla_{\\theta}E_{\\theta}(\\boldsymbol{x})]\n",
    "$$\n",
    "\n",
    "これは，パラメータ$\\theta$についての勾配．\n",
    "\n",
    "分配関数$Z_\\theta$は，$\\theta$に依存する関数なので，$\\theta$についての勾配を取ったときに，分配関数は残ってくる．\n",
    "\n",
    "つまり，分配関数について考えない距離関数を考えれば良い．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "そこでSBMでは，フィッシャーダイバージェンスを採用する．\n",
    "\n",
    "フィッシャーダイバージェンス\n",
    "\n",
    "$$\n",
    "\\frac{1}{2}\\mathbb{E}_{p_{data}(\\boldsymbol{x})}[||\\nabla_{\\boldsymbol{x}}\\log p_{\\theta}(\\boldsymbol{x})-\\nabla_{\\boldsymbol{x}}\\log p_{data}(\\boldsymbol{x})||_2^2]\n",
    "$$\n",
    "\n",
    "みてすぐわかるとおり，フィッシャーダイバージェンスは$\\boldsymbol{x}$についての勾配なので，$\\theta$に依存しない．\n",
    "\n",
    "つまり，勾配を取った時に分配関数について考えなくて良い．ということになる．\n",
    "\n",
    "※ $\\nabla_{\\boldsymbol{x}} Z_{\\theta}=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{1}{2}\\mathbb{E}_{p_data(x)}[||s_{\\theta}(x) - \\nabla_x \\log p_{data}(x)||_2^2] &= \\frac{1}{2}\\mathbb{E}_{p_{data(x)}}[(s_{\\theta}(x) - \\nabla_x \\log p_{data(x)})^T(s_{\\theta}(x) - \\nabla_x \\log p_{data(x)})]\\\\\n",
    "&= \\frac{1}{2}\\mathbb{E}_{p_{data}(x)}[s_{\\theta}(x)^T s_{\\theta}(x) - s_{\\theta}(x)^T \\nabla_x \\log p_{data}(x) - \\nabla_x \\log p_{data}(x)^T s_{\\theta}(x) + \\nabla_x \\log p_{data}(x)^T \\nabla_x \\log p_{data}(x)]\\\\\n",
    "&= \\frac{1}{2}\\mathbb{E}_{p_{data}(x)}[s_{\\theta}(x)^T s_{\\theta}(x) - s_{\\theta}(x)^T \\nabla_x \\log p_{data}(x) - s_{\\theta}(x)^T \\nabla_x \\log p_{data}(x) + \\nabla_x \\log p_{data}(x)^T \\nabla_x \\log p_{data}(x)]\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "内積は対象なので，$s_{\\theta}(x)^T \\nabla_x \\log p_{data}(x) = \\nabla_x \\log p_{data}(x)^T s_{\\theta}(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ノルム表記に戻す\n",
    "\n",
    "$$\n",
    "= \\frac{1}{2}\\mathbb{E}_{p_{data}(x)}[||s_{\\theta}(x)||_2^2 - s_{\\theta}(x)^T \\nabla_x \\log p_{data}(x) - s_{\\theta}(x)^T\\nabla_x \\log p_{data}(x) + ||\\nabla_x \\log p_{data}(x)||_2^2]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上のことを踏まえて式を整理する．\n",
    "$$\n",
    "\\begin{align*}\n",
    "&= \\mathbb{E}_{p_{data}(x)}[||\\frac{1}{2}s_{\\theta}(x)||_2^2 - \\frac{1}{2}2s_{\\theta}(x)^T \\nabla_x \\log p_{data}(x) + \\frac{1}{2}||\\nabla_x \\log p_{data}(x)||_2^2]\\\\\n",
    "&= \\mathbb{E}_{p_{data}(x)}[||\\frac{1}{2}s_{\\theta}(x)||_2^2 - s_{\\theta}(x)^T \\nabla_x \\log p_{data}(x) + \\frac{1}{2}||\\nabla_x \\log p_{data}(x)||_2^2]\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また，期待値内の第2項 $s_{\\theta}(x)^T \\nabla_x \\log p_{data}(x)$ に対して、次の恒等式が使えます。これは確率論と微分積分の結果で、スコア関数の性質を活用しています\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{p_{data}(x)}[s_{\\theta}(x)^T \\nabla_x \\log p_{data}(x)] = -\\mathbb{E}_{p_{data}(x)}[tr(\\nabla_x s_{\\theta}(x))]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これを使う．\n",
    "$$\n",
    "\\mathbb{E}_{p_{data}(x)}[||\\frac{1}{2}s_{\\theta}(x)||_2^2 + tr(\\nabla_x s_{\\theta}(x)) + \\frac{1}{2}||\\nabla_x \\log p_{data}(x)||_2^2]\n",
    "$$\n",
    "\n",
    "また，$\\frac{1}{2}||\\nabla_x \\log p_{data}(x)||_2^2$はデータ分布に依存する値であり，$\\theta$には依存しない値なので定数として扱える．したがって$C$とおける．\n",
    "\n",
    "最終的に以下のようになる．\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{p_{data}(x)}[||\\frac{1}{2}s_{\\theta}(x)||_2^2 + tr(\\nabla_x s_{\\theta}(x))] + C\n",
    "$$\n",
    "\n",
    "この式の第一項を，$\\theta$について最適化することで生成モデルを学習できる．データ分布に依存する値を$C$としておいたのは，単純に最適化する時に定数は考えなくて良いからである．\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{p_{data}(x)}[||\\frac{1}{2}s_{\\theta}(x)||_2^2 + tr(\\nabla_x s_{\\theta}(x))]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "意味もわからず変形したが，$tr(\\nabla_x s_{\\theta}(x))$はトレースと言う．\n",
    "\n",
    "また，スコア関数は，元々一階微分されているものなので，トレースの中身は二階微分になっている．\n",
    "\n",
    "つまり，ヘッセ行列のトレースを計算する必要があるということ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "しかし，スコア関数の勾配のトレース．\n",
    "\n",
    "つまり対数尤度のヘッセ行列のトレースを計算するということ．この計算にはとてもコストがかかる．\n",
    "\n",
    "スコアベースモデルでは，ここのヘッセ行列のトレースの計算をいかにして回避するかが，鍵になり．さまざまな手法が提案されている．"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
